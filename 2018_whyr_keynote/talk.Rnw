\documentclass[10pt]{beamer}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{array}
\usepackage{adjustbox}
\usepackage{xspace}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,backgrounds,fit,positioning,chains,shadows,decorations.pathmorphing,decorations.pathreplacing,matrix}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{wasysym}
\usepackage[binary-units=true]{siunitx}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{dsfont}

\definecolor{tugreen}{cmyk}{0.57, 0, 1.00, 0}
\definecolor{tugreen1}{cmyk}{0.57, 0, 1.00, 0}
\definecolor{tugreen2}{HTML}{667E4D}
\definecolor{tugreen3}{HTML}{72A544}
\definecolor{tugreen4}{HTML}{3A472E}

\usecolortheme{dove}
\usetheme{boxes}
\usefonttheme{structuresmallcapsserif}
\newenvironment{whiteframe}
{
 \usebackgroundtemplate{}
 \begin{frame}
}
{
 \end{frame}
}

\usetikzlibrary{shapes,matrix,positioning,chains,arrows,shadows,decorations.pathmorphing,fit,backgrounds}
\setbeamercolor{itemize item}{fg=tugreen1}
\setbeamercolor{itemize subitem}{fg=tugreen1}
\setbeamertemplate{itemize item}[square]
\setbeamertemplate{footline}[frame number]
\beamertemplatenavigationsymbolsempty

\title{Machine Learning in R: Package \texttt{mlr}}
\logo{\includegraphics[scale=0.05]{mlr}}
\author{Bernd~Bischl\\ Computational Statistics, LMU}
\titlegraphic{\includegraphics[height=.3\textheight]{mlr}}
\date{}

\newcommand{\norm}[2][\relax]{\ifx#1\relax\ensuremath{\left\Vert#2\right\Vert}\else\ensuremath{\left\Vert#2\right\Vert_{#1}}\fi}
\newcommand{\ind}{\mathds{1}}
\newcommand{\pred}[1]{\ind\left(#1\right)}
\newcommand{\abs}[1]{\ensuremath{\left| #1 \right|}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\texttt{#1}}
\newcommand{\tarrow}{\textcolor{tugreen1}{{\ding{212}}}\xspace}

% suppress frame numbering, so noframenumbering works
% \setbeamertemplate{frametitle continuation}
%   \begin{frame}[containsverbatim,allowframebreaks,noframenumbering]

\newenvironment{vframe}
{
  \begin{frame}[containsverbatim]
}
{
 \end{frame}
}

\newenvironment{vbframe}
{
  \begin{frame}[containsverbatim,allowframebreaks]
}
{
 \end{frame}
}

\newenvironment{blocki*}
{
  \begin{block}{}\begin{itemize}
}
{
\end{itemize}\end{block}
}

\newenvironment{blocki}[1]
{
  \begin{block}{#1}\begin{itemize}
}
{
\end{itemize}\end{block}
}

\newcommand{\oneliner}[1]{\begin{block}{}\begin{center}\begin{Large}#1\end{Large}\end{center}\end{block}}


\renewcommand<>{\sout}[1]{
  \only#2{\beameroriginal{\sout}{#1}}
  \invisible#2{#1}
}


\AtBeginSection{\frame{\sectionpage}}


\begin{document}
% \usebackgroundtemplate{
%   \begin{tikzpicture}
%     \shade [inner color = white, outer color = gray!30, opacity = 0.8] (\paperwidth,\paperheight) rectangle (0,0);
%     \shade [inner color = white, outer color = gray!10, opacity=.05] (\paperwidth/2,\paperheight/2) circle (3);
%   \end{tikzpicture}
% }

<<opts,include=FALSE,cache=FALSE>>=
library(knitr)
library(BBmisc)
library(mlr)
library(ggplot2)
library(parallelMap)
library(tikzDevice)
library(data.table)
library(gridExtra)
library(survMisc)
options(width = 70)
configureMlr(show.info = FALSE)
configureMlr(show.learner.output = FALSE)
OPENML_EVAL = TRUE

knit_hooks$set(document = function(x) {
  # silence xcolor
  x = sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)
  # add an noindent after hooks -> remove blank line
  x = gsub('(\\\\end\\{knitrout\\}[\n]+)', '\\1\\\\noindent ', x)
  x
})

opts_chunk$set(
   fig.path = "knitr/figures/",
   cache.path = "knitr/cache/",
   cache = TRUE,
   tidy = FALSE,
#   dev = 'tikz',
   external = TRUE,
   fig.align = "center",
   size = "scriptsize",
   stop = TRUE,
   fig.width = 9 * 0.8,
   fig.height = 6 * 0.8,
   small.mar = TRUE,
   prompt = TRUE
)
@

%% PART I
\begin{frame}
  \titlepage
\end{frame}

\begin{vframe}{About}
  \begin{itemize}
    \item Project home page\\
    \oneliner{\url{https://github.com/mlr-org/mlr}}
    \item \textbf{Tutorial} for online viewing / download, including many examples
    \item 8-10 main developers, quite a few contributors, 4 GSOC projects in 2015/16 and
    one in 2017
    \item About 30K lines of code, 8K lines of unit tests
    \item Need help? Ask on stackoverflow with tag \textit{mlr} or open an issue
  \end{itemize}
\end{vframe}


\begin{vframe}{Motivation}
  \begin{blocki}{The good news}
  \item CRAN serves hundreds of packages for machine learning
    % (cf.\ CRAN task view machine learning)
  \item Often compliant to the unwritten interface definition:
<<model-standard,eval=FALSE>>=
model = fit(target ~ ., data = train.data, ...)
predictions = predict(model, newdata = test.data, ...)
@
  \end{blocki}
% \end{vframe}

% \begin{vframe}{Motivation}
  \begin{blocki}{The bad news}
    \item Some packages API is \enquote{just different}
    \item Functionality is always package or model-dependent, even though the procedure might be general
    \item No meta-information available or buried in docs
      % (sometimes not documented at all)
    % \item Many packages require the user to \enquote{guess} good hyperparameters
    % \item Result: engthy, tedious and error-prone code
  \end{blocki}
  \oneliner{Our goal: A domain-specific language for many machine learning concepts!}
\end{vframe}



\begin{frame}{Building Blocks}
  \begin{center}
    \includegraphics[width=0.9\textwidth]{figure/ml_abstraction-crop.pdf}
  \end{center}
  \begin{itemize}
    \item \pkg{mlr} objects: tasks, learners, measures, resampling instances.
  \end{itemize}
\end{frame}


\begin{vframe}{Motivation: \pkg{mlr}}
  \begin{itemize}
    \item Clean and extensible via S3
    \item Reflections: nearly all objects are queryable (i.e.\ you can ask them for their properties and program on them)
    \item The OO-structure allows many generic algorithms
      \begin{itemize}
        \item Resampling
        \item Tuning
        \item Feature selection
        \item Wrapping / Pipelining
        \item Nested Resampling
        \item \ldots
      \end{itemize}
  \end{itemize}
\end{vframe}




<<gatherSummary,include=FALSE>>=
ee = as.environment("package:mlr")
nl = table(sub("^makeRLearner\\.([[:alpha:]]+)\\..+", "\\1", methods("makeRLearner")))
nm = sapply(list(classif = listMeasures("classif"), regr = listMeasures("regr"), surv = listMeasures("surv"), cluster = listMeasures("cluster")), length) - 4
@


\begin{vframe}{Task Abstraction}
  \begin{itemize}
    \item Classification
    \item Regression
    \item Survival analysis
    \item Clustering
    \item Multi-Label
    \item Cost-Sensitive learning
    \item Functional Data
  \end{itemize}
\end{vframe}


% \begin{vbframe}{Task Abstraction: API}
% <<task2>>=
% getTaskId(task)
% str(getTaskData(task))
% @
% \framebreak
% <<task3>>=
% str(getTaskDescription(task))
% @
% \framebreak
% <<task4>>=
% getTaskSize(task)
% getTaskFeatureNames(task)
% getTaskTargetNames(task)
% getTaskFormula(task)
% summary(getTaskTargets(task))
% @
% \end{vbframe}


% \begin{vframe}{Learner Abstraction}
%   \begin{itemize}
%     \item Internal structure of learners:
%       \begin{itemize}
%         \item wrappers around \code{fit()} and \code{predict()} of the package
%         \item description of the parameter set
%         \item annotations
%       \end{itemize}
%     \item Naming convention: \texttt{<tasktype>.<functionname>}\\
%       e.g.: classif.svm, regr.lm
% % <<naming-convention,eval=FALSE>>=
% % makeLearner("classif.rpart")
% % makeLearner("regr.rpart")
% % @
%     % \item For the lazy: Instead of creating the object, you can also pass a string to nearly
%     %   every function that takes a learner
%     % \item Reduction algorithms for cost-sensitive classification
%     \item Adding custom learners is covered in the tutorial
%   \end{itemize}
% % \framebreak
% <<learner1>>=
% lrn = makeLearner("classif.svm", predict.type = "prob", kernel = "linear", cost = 1)
% print(lrn)
% @
% \end{vframe}

\begin{vframe}{Learner Abstraction}
  \begin{scriptsize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{blocki}{Classification (\Sexpr{nl["classif"]})}
        \item LDA, QDA, RDA, MDA
        \item Trees and forests
        \item Boosting (different variants)
        \item SVMs (different variants)
        \item Deep Neural Networks
        \item \ldots
    \end{blocki}
    \begin{blocki}{Clustering (\Sexpr{nl["cluster"]})}
        \item K-Means
        \item EM
        \item DBscan
        \item X-Means
        \item \ldots
    \end{blocki}
    \column{0.4\textwidth}
    \begin{blocki}{Regression (\Sexpr{nl["regr"]})}
        \item Linear, lasso and ridge
        \item Boosting
        \item Trees and forests
        \item Gaussian processes
        \item Deep Neural Networks
        \item \ldots
    \end{blocki}
    \begin{blocki}{Survival (\Sexpr{nl["surv"]})}
        \item Cox-PH
        \item Cox-Boost
        \item Random survival forest
        \item Penalized regression
        \item \ldots
    \end{blocki}
  \end{columns}
  \end{scriptsize}
  \oneliner{We can explore them on the webpage -- or ask \pkg{mlr}}


% <<listlrns1, warning=FALSE>>=
% # list all classification learners which can predict probabilities
% # and allow multiclass classification
% listLearners("classif",
%   properties = c("prob", "multiclass"))[1:5, c(-2, -5, -16)]
% @

% \framebreak

% \oneliner{Get all applicable learners for a task}
% <<listlrns2>>=
% listLearners(task)[1:5, c(-2, -5, -16)]
% @

\end{vframe}

\begin{vframe}{Parameter Abstraction}
  \begin{itemize}
    \item Extensive meta-information for hyperparameters available:\\
      storage type, constraints, defaults, dependencies
    \item Automatically checked for feasibility
    \item You can program on parameters!
    \end{itemize}
<<parmset, size='tiny', echo=4>>=
w = getOption("width")
lrn = makeLearner("classif.svm", predict.type = "prob", kernel = "linear", cost = 1)
options(width = 160)
getParamSet(lrn)
options(width = w)
@
\end{vframe}


\begin{vframe}{Resampling Abstraction}

  \begin{itemize}
    \item Procedure: Train, Predict, Eval, Repeat.
    \item Aim: Estimate expected model performance.
      \begin{itemize}
        \item Hold-Out
        \item Cross-validation (normal, repeated)
        \item Bootstrap (OOB, B632, B632+)
        \item Subsampling
        \item Stratification
        \item Blocking
      \end{itemize}
    \item Benchmarking / Model comparison with one command
  \end{itemize}

\end{vframe}

\begin{vframe}{Configuring the Package}

\begin{blocki*}
  \item What to do when training fails? error, warn, or be quiet?\\
    \tarrow You don't want to stop in complex loops like \code{benchmark}\\
    \tarrow \code{FailureModel} is created that predicts NAs
  \item Show verbose info messages?
  \item What if parameters are not described in learner?
  \item \code{?configureMlr} sets global flags and can be overwritten for individual learners
\end{blocki*}
\end{vframe}



\begin{vframe}{Hyperparameter Tuning}
  \begin{blocki}{Tuning}
  \item Find \enquote{best} hyperparameters data-dependently
  \item Tuner proposes config, eval by resampling, feedback to tuner
  \end{blocki}
  
  \begin{center}
    \includegraphics[width=0.65\textwidth]{figure/ml_abstraction_optimization-crop.pdf}
  \end{center}

\end{vframe}


% \begin{vframe}{Automatic Model Selection}
%   \begin{blocki}{Prior approaches:}
%   \item Looking for the silver bullet model \\
%     $\leadsto$ Failure\\
%   \item Exhaustive benchmarking / search \\
%     $\leadsto$ Per data set: too expensive \\
%     $\leadsto$ Over many: contradicting results
%   \item Meta-Learning:\\
%     $\leadsto$ Failure \\
%     $\leadsto$ Usually not for preprocessing / hyperparamters
%   \end{blocki}

%   \structure{Goal}: Data dependent + Automatic + Efficient
% \end{vframe}

% \begin{frame}{Adaptive tuning}
  % \begin{center}
    % \includegraphics[width=0.85\textwidth]{figure/ml_abstraction_optimization-crop.pdf}
  % \end{center}
% \end{frame}

\begin{vframe}{Implemented Tuning Techniques}
\begin{itemize}
  \item Grid Search
  \item Random Search
  \item Simulated Annealing
  \item Evolutionary Algorithms / CMAES
  \item Iterated F-Racing
  \item Model-based Optimization / Bayesian Optimization
\end{itemize}
\end{vframe}


\begin{frame}{mlrMBO: Model-Based Optimization Toolbox}
\begin{minipage}{0.4\linewidth}
    \begin{itemize}
      \item Any regression from mlr
      \item Arbtritrary infill
      \item Single - or multi-crit
      \item Multi-point proposal
      \item Via parallelMap and batchtools
        runs on many parallel backends and clusters
      \item Algorithm configuration
      \item Active research
    \end{itemize}

\end{minipage}
\begin{minipage}{0.55\linewidth}
    \includegraphics[width = \textwidth]{figure/mlrMBO1.pdf}
\end{minipage}
\begin{center}
    \begin{itemize}
      \item mlrMBO:
        \url{https://github.com/mlr-org/mlrMBO}
      \item mlrMBO Paper on arXiv (under review)
        \url{https://arxiv.org/abs/1703.03373}
    \end{itemize}
\end{center}
\end{frame}

\begin{vframe}{Parallelization} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
    \item We use our own package: \pkg{parallelMap}
    \item Setup:
<<parallelMap,eval=FALSE>>=
parallelStart("multicore")
benchmark(...)
parallelStop()
@
    \item Backends: \texttt{local}, \texttt{multicore}, \texttt{socket}, \texttt{mpi} and \texttt{batchtools}
    \item The latter means support for: makeshift SSH-clusters, Docker swarm and HPC schedulers like SLURM, Torque/PBS, SGE or LSF
  \item Levels allow fine grained control over the parallelization
    \begin{itemize}
  \item \code{mlr.resample}: Job = \enquote{train / test step}
  \item \code{mlr.tuneParams}: Job = \enquote{resample with these parameter settings}
  \item \code{mlr.selectFeatures}: Job = \enquote{resample with this feature subset}
    \item \code{mlr.benchmark}: Job = \enquote{evaluate this learner on this data set}
    \end{itemize}
    \end{itemize}

\end{vframe}

\begin{vbframe}{\pkg{mlr} Learner Wrappers}
  \begin{blocki}{What?}
    \item Extend the functionality of learners by adding an \pkg{mlr} wrapper to them
    \item The wrapper hooks into the train and predict of the base learner and extends it
    \item This way, you can create a new \pkg{mlr} learner with extended functionality
    \item Hyperparameter definition spaces get joined!
  \end{blocki}
  \framebreak
  \begin{blocki}{Available Wrappers}
    \item \structure{Preprocessing}: PCA, normalization (z-transformation)
    \item \structure{Parameter Tuning}: grid, optim, random search, genetic algorithms, CMAES, iRace, MBO
    \item \structure{Filter}: correlation- and entropy-based, $\mathcal{X}^2$-test, mRMR, \ldots
    \item \structure{Feature Selection}: (floating) sequential forward/backward, exhaustive search, genetic algorithms, \ldots
    \item \structure{Impute}: dummy variables, imputations with mean, median, min, max, empirical distribution or other learners
    \item \structure{Bagging} to fuse learners on bootstraped samples
    \item \structure{Stacking} to combine models in heterogenous ensembles
    \item \structure{Over- and Undersampling} for unbalanced classification
  \end{blocki}
\end{vbframe}

\begin{vframe}{Nested Resampling}
  \begin{itemize}
    \item Using the TuningWrapper or FeatureSelectionWrapper allows to enable nested resampling
    \item Ensures \textbf{unbiased} results for model optimization
    \item Everything else is statistically unsound
  \end{itemize}
  \begin{center}
    \includegraphics[width=8cm]{figure/nested.png}
  \end{center}
\end{vframe}


% \begin{vframe}{Visualizations}
%   \begin{itemize}
%     % \item A brief time ago, \pkg{mlr} was pretty bare-bones here
%     \item We use \pkg{ggplot2} and interactive \pkg{ggvis} as a standard, if possible
%     \item Some plots use Viper Charts as backend (cost curves, lift charts, \ldots)
%     \item GSOC project 2015 with Zach Jones
%     \begin{itemize}
%       \item Demo plots for models in teaching
%       \item ROC curves
%       \item Threshold vs. Performance
%       \item Partial dependency plot
%       \item Learning curves
%     \end{itemize}
%   \end{itemize}
% \end{vframe}

% \begin{vframe}{R Example}
%   \oneliner{Visualizations}
% \end{vframe}


% \begin{vbframe}{caret vs. mlr}


% \oneliner{Of course we are biased :)}

% \begin{blocki}{Why is caret great}
% \item caret is an overall great package
% \item caret has much better visibility
%   (This sucks. We will work hard on changing this)
% \item caret has a book (I guess we won't -- soon)
% \item caret has a few more regression and classification learners
% \item caret has (rudimentary) support for time-series data\\
%   (\pkg{mlr} will have that soon)
% \end{blocki}
% \framebreak

% % \begin{blocki}{Why we like \pkg{mlr} more}
% % % \item \pkg{mlr} is under active development by a substantial number of people: 6 core developers, several other packages with lots of interoperability, and 3 GSOC 2015 developers
% % \item \pkg{mlr} has (in our opinion) a better OO design, class structure and infrastructure for future development and for users
% % \item This makes things combinable, extensible, predictable
% % \item More flexible parallelization with \pkg{parallelMap}, even on HPCs via \pkg{BatchJobs}
% % \item Tuning with advanced methods like \pkg{irace}
% % \item  Fusing learners with other operations like pre-processing and feature selection
% % % \item \pkg{mlr} has better infrastructure for training, evaluating, tuning, benchmarking, and visualizing learning algorithms
% % % \item \pkg{mlr} has (vastly) better unit tests (which means it is more reliable!).
% % % \item \pkg{mlr} has better software documentation
% % % \item \pkg{mlr} has a consistent style
% % \item Nested resampling (required for unbiased results)
% % \item Survival and cluster analysis
% % % \item \textbf{With more visibility, funding, and contributors it could be even more awesome}
% % \end{blocki}

% % % \framebreak

% % % \begin{blocki}{What \pkg{mlr} can do, but caret cannot}
% % % \item Blocking in resampling
% % % \item Integrated stacking (a new separate package for caret, \pkg{caretEnsemble})
% % % \item Partial predictions/dependence for any supervised method
% % % \item Cost-sensitive learning
% % % \end{blocki}
% % \end{vbframe}

\begin{vbframe}{autoxgboost}

\centering
\begin{figure}
\includegraphics[width=1.0\textwidth]{figure/autoxgboost_schema.png}
\end{figure}

available @ Github: \url{https://github.com/ja-thomas/autoxgboost}

\framebreak


\begin{itemize}
\item Only a task is require
\item Model-based hyperparameter tuning
\item Threshold optimization
\item Encoding of categorical features
\item Handles missing values
\end{itemize}

\framebreak

<<autoxgb, eval=TRUE, message=FALSE>>=
library(autoxgboost)
titanic = read.csv("titanic_train.csv")
task = makeClassifTask("titanic", titanic, target = "Survived")
autoxgboost(task, iterations = 10L)
@

\end{vbframe}

\begin{vframe}{There is more \ldots}
  \begin{blocki*}
  \item ROC and learning curves
  \item Imbalancy correction
  \item Multi-Label learning
  \item Multi-criteria optimization
  \item Ensembles, generic bagging and stacking
  \item \ldots
  \end{blocki*}
\end{vframe}
\begin{vframe}{Outlook}
  \begin{blocki}{We are working on}
  \item mlr2 - nextgen
  \item Composable preprocessing operators: mlrCPO
  \item Learning defaults automatically
  \item \ldots
  \end{blocki}
\end{vframe}
% \begin{vframe}
  % \oneliner{Thanks!}

% Useful Links:

% \vfill

% \begin{itemize}
%   \item \url{https://github.com/mlr-org/mlr}
%   \item \url{https://mlr-org.github.io/}
%   \item \url{https://mlr-org.github.io/mlr-tutorial/devel/html/}
%   \item \url{https://github.com/mlr-org/mlrCPO}
%   \item \url{https://github.com/mlr-org/mlrMBO}
%   \item \url{https://github.com/openml/openml-r}
% \end{itemize}
% \end{vframe}


\end{document}
% vim: set spelllang=en :
